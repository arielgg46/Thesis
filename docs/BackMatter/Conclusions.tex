\begin{conclusions}
La presente tesis tuvo como objetivo general diseñar, implementar y evaluar agentes basados en \textit{Large Language Models (LLMs)} con técnicas propuestas para mejorar la correctitud sintáctica y semántica de la generación de códigos \textit{PDDL} en modelación de tareas de planificación, además de realizar un análisis comparativo de los agentes implementados para determinar los factores que influyen positivamente en métricas clave como validez sintáctica, solubilidad y correctitud. Este objetivo se basó en las limitaciones y oportunidades observadas en la revisión de la literatura, específicamente: la potencial futilidad del paradigma \textit{LLM-as-Planner}; el cuello de botella que representa la fase de modelado de los problemas de planificación de los algoritmos clásicos; las limitaciones de los trabajos actuales situados en el paradigma \textit{LLM-as-Modeler}, que son la presencia de errores sintácticos, inconsistencias semánticas y fuerte dependencia de conocimiento, supervisión y labor experta; el potencial de los métodos que inducen razonamiento intermedio en los \textit{LLMs}, la técnica de \textit{Grammar-Constrained Decoding (GCD)} y los mecanismos de aprendizaje experiencial introducidos por el trabajo de \textit{ExpeL}. A lo largo del trabajo se cumplieron todos los objetivos específicos planteados, a través de una propuesta sólida, una implementación exhaustiva y una evaluación empírica rigurosa, aunque limitada por restricciones computacionales, económicas y temporales.

En primer lugar, se realizó un estudio profundo del estado del arte, que permitió identificar las limitaciones actuales de los enfoques basados en \textit{LLMs} para la planificación automática, en particular la falta de robustez sintáctica y semántica de los modelos generados, así como la ausencia de mecanismos que guíen o estructuren el razonamiento del agente. A partir de este análisis, se propuso un agente modelador de problemas de planificación capaz de generar automáticamente modelos \textit{PDDL} válidos a partir de descripciones en lenguaje natural, valiéndose de técnicas como razonamiento estructurado, extracción de objetos, \textit{Few-Shot Prompting} (FSP), \textit{Retrieval-Augmented Generation} (RAG), aprendizaje experiencial con extracción de \textit{insights} y reflexión, y generación controlada por gramática con \textit{GCD} y una variante especializada introducida denominada \textit{DAPS}.

La implementación del agente fue acompañada de una estructura modular y extensible, incluyendo las variantes experimentales de cada uno de sus componentes, así como los \textit{baselines} de la literatura con los que se comparó el desempeño. Se construyó además un \textit{pipeline} completo de entrenamiento, evaluación y análisis de los agentes, junto a un conjunto de herramientas auxiliares para la generación y validación automática de archivos \textit{PDDL}. Todo este desarrollo fue publicado en un repositorio de \textit{GitHub} (\href{https://github.com/arielgg46/Thesis}{github.com/arielgg46/Thesis}), documentado y reproducible, lo cual constituye uno de los principales aportes prácticos de esta tesis para la comunidad investigadora. Además, se identificaron errores en la generación del \textit{dataset Planetarium}, y se propusieron e implementaron correcciones, ayudando a garantizar la correctitud y utilidad de tan importante \textit{benchmark} para la evaluación de agentes modeladores de problemas de planificación.

Durante la etapa de evaluación, se diseñó un experimento basado en subconjuntos estratificados del \textit{benchmark Planetarium}, seleccionados para balancear las distintas dimensiones del espacio de problemas (como dominios, estructuras del estado inicial y metas, cantidad de objetos y predicados, etc.). Se evaluaron 13 agentes en un total de 70 tareas de prueba, midiendo su desempeño en cuanto a validez sintáctica, solubilidad y correctitud, y se realizó un análisis incremental para determinar el impacto de cada componente individual y de sus combinaciones.

A pesar de las limitaciones experimentales derivadas del alto costo computacional, económico y temporal de la evaluación a gran escala, el estudio ofreció evidencia empírica clara sobre la efectividad de los módulos propuestos. En particular:

\begin{itemize}
    \item La división del proceso de modelado en fases estructuradas permitió mejoras sustanciales en la correctitud semántica, facilitando el razonamiento modular y verificable del agente. Específicamente, en métricas de validez sintáctica, solubilidad y correctitud, se pasó respectivamente de \textbf{\textcolor[rgb]{0.0,0.87,0.0}{87.14 \%}}, \textbf{\textcolor[rgb]{0.0,0.54,0.0}{54.29 \%}} y \textbf{\textcolor[rgb]{0.0,0.23,0.0}{22.86 \%}} en el \textit{baseline Zero-Shot} reimplementado (\texttt{llm\_plus\_p}) a \textbf{\textcolor[rgb]{0.0,0.84,0.0}{84.29 \%}}, \textbf{\textcolor[rgb]{0.0,0.79,0.0}{78.57 \%}} y \textbf{\textcolor[rgb]{0.0,0.43,0.0}{42.86}} \% con la inclusión de estos módulos en \texttt{r\_o}; y de \textbf{\textcolor[rgb]{0.0,0.87,0.0}{87.14 \%}}, \textbf{\textcolor[rgb]{0.0,0.83,0.0}{82.86 \%}} y \textbf{\textcolor[rgb]{0.0,0.57,0.0}{57.14 \%}} en el \textit{baseline One-Shot} reimplementado (\texttt{llm\_plus\_p\_fsp}) a \textbf{\textcolor[rgb]{0.0,0.91,0.0}{91.43 \%}}, \textbf{\textcolor[rgb]{0.0,0.89,0.0}{88.57 \%}} y \textbf{\textcolor[rgb]{0.0,0.76,0.0}{75.71 \%}} en \texttt{r\_o\_fsp}. Esta conclusión respalda la hipótesis \textbf{H1} en el contexto limitado evaluado.

    \item La aplicación de \textit{GCD}, en concreto en su forma especializada \textit{DAPS}, eliminó por completo los errores sintácticos en los experimentos realizados, validando empíricamente la hipótesis \textbf{H2} dentro del alcance de esta tesis. Se partió de una validez sintáctica de \textbf{\textcolor[rgb]{0.0,0.87,0.0}{87.14 \%}} en el \textit{baseline} reimplementado a una de \textbf{\textcolor[rgb]{0.0,1.0,0.0}{100 \%}} en modalidad \textit{One-Shot}.

    \item El agente que integró todos los componentes propuestos alcanzó los mayores valores en las métricas clave: \textbf{\textcolor[rgb]{0.0,1.0,0.0}{100\,\%}} de validez sintáctica, \textbf{\textcolor[rgb]{0.0,0.87,0.0}{87.14\,\%}} de solubilidad y \textbf{\textcolor[rgb]{0.0,0.81,0.0}{81.43\,\%}} de correctitud, dentro del conjunto de evaluación construido, superando ampliamente tanto a sus versiones parciales como a los \textit{baselines} extraídos de la literatura. Se cerraron efectivamente el \textbf{\textcolor[rgb]{0.0,1.0,0.0}{100 \%}}, \textbf{\textcolor[rgb]{0.0,0.67,0.0}{66.68 \%}} y \textbf{\textcolor[rgb]{0.0,0.57,0.0}{56.67 \%}} de los márgenes de mejora del \textit{baseline} \texttt{llm\_plus\_p\_fsp} en las métricas respectivas.

    \item Al incorporar mecanismos de reflexión y retroalimentación automática (\texttt{exp}), se corrigieron errores residuales, elevando la solubilidad a \textbf{\textcolor[rgb]{0.0,0.94,0.0}{98.57\,\%}} y la correctitud a \textbf{\textcolor[rgb]{0.0,0.84,0.0}{84.29\,\%}}, lo que valida parcialmente la hipótesis \textbf{H3}. Finalmente, la combinación sinérgica de reflexión, reintentos, recuperación de ejemplos previos e \textit{insights} construidos manualmente (\texttt{exp\_rag\_hi}) permitió alcanzar los valores más altos de todo el estudio: \textbf{\textcolor[rgb]{0.0,1.0,0.0}{100\,\%}} de validez sintáctica, \textbf{\textcolor[rgb]{0.0,1.0,0.0}{100\,\%}} de solubilidad y \textbf{\textcolor[rgb]{0.0,0.87,0.0}{87.14\,\%}} de correctitud, respaldando empíricamente también la hipótesis \textbf{H4}. Este agente logró superar a \texttt{r\_o\_daps\_gcd\_fsp}, incluso en dominios complejos, como evidencia del potencial de integrar mecanismos de conocimiento reutilizable en el modelado automático.

\end{itemize}

No obstante, los resultados deben interpretarse dentro de los márgenes de validez impuestos por la evaluación. Las tareas utilizadas pertenecen a tres dominios clásicos del \textit{benchmark Planetarium}, lo que limita la generalización a contextos más complejos o realistas. La combinación de módulos fue evaluada de forma incremental y no exhaustiva, lo cual no permite aislar por completo el efecto de cada técnica. Además, las decisiones de diseño de \textit{prompts}, \textit{LLMs} base, límites de operaciones e interacciones con los \textit{LLMs} estuvieron sujetas a restricciones prácticas, sin exploración sistemática de hiperparámetros. Adicionalmente, la fase de extracción de \textit{insights} no obtuvo el desempeño esperado, lo que limitó la validación de \textbf{H4} a la evaluación usando una base de \textit{Human Insights}.

Pese a estas limitaciones, los resultados de la tesis constituyen un aporte significativo al campo del modelado automático con \textit{LLMs} en planificación. La propuesta demuestra que es posible integrar múltiples técnicas complementarias para construir agentes robustos y adaptativos, capaces de generar modelos válidos y funcionales a partir de descripciones en lenguaje natural, con una tasa de éxito comparable a enfoques manuales o guiados por expertos humanos.

Este trabajo deja como legado no sólo una arquitectura funcional de agente modelador, sino también una base experimental, un marco de evaluación y un conjunto de herramientas replicables que sientan las bases para investigaciones futuras. 

En conclusión, esta tesis cumple satisfactoriamente sus objetivos propuestos, aportando evidencia, herramientas y desarrollos que consolidan el uso de \textit{LLMs} como una tecnología prometedora para la generación automática de modelos de planificación, y abre un camino viable hacia agentes más inteligentes, precisos y generalizables en tareas de modelado cognitivo.
\end{conclusions}
